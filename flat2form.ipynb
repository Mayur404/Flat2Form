{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0529f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU (training will be slower)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def process_hypersim_scene(scene_path, output_dir='processed_data'):\n",
    "    scene_path = Path(scene_path)\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Create output directories\n",
    "    (output_dir / 'rgb').mkdir(parents=True, exist_ok=True)\n",
    "    (output_dir / 'depth').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Processing scene: {scene_path}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Find all set directories (set1, set2, set3, ...)\n",
    "    set_dirs = [d for d in scene_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if not set_dirs:\n",
    "        print(\"ERROR: No set directories found!\")\n",
    "        return []\n",
    "    \n",
    "    processed_samples = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for set_dir in tqdm(set_dirs, desc=\"Processing sets\"):\n",
    "        print(f\"Processing set directory: {set_dir}\")\n",
    "        \n",
    "        # Find all depth files (frame.XXXX.depth_meters.png)\n",
    "        depth_files = sorted(set_dir.glob(\"frame.*.depth_meters.png\"))\n",
    "        print(f\"Found {len(depth_files)} depth files in {set_dir}\")\n",
    "        \n",
    "        if len(depth_files) == 0:\n",
    "            print(f\"ERROR: No depth files found in {set_dir}!\")\n",
    "            continue\n",
    "        \n",
    "        for depth_file in tqdm(depth_files, desc=f\"Processing frames in {set_dir.name}\"):\n",
    "            try:\n",
    "                # Extract frame number from filename (e.g., frame.0000.depth_meters.png -> 0000)\n",
    "                frame_num = depth_file.stem.split('.')[1]\n",
    "                \n",
    "                # Find corresponding RGB image (frame.XXXX.color.jpg)\n",
    "                rgb_file = set_dir / f\"frame.{frame_num}.color.jpg\"\n",
    "                \n",
    "                if not rgb_file.exists():\n",
    "                    print(f\"Warning: No RGB file for frame {frame_num}, skipping...\")\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Load RGB\n",
    "                rgb = Image.open(rgb_file).convert('RGB')\n",
    "                \n",
    "                # Load depth (PNG format - already in meters)\n",
    "                depth_img = Image.open(depth_file)\n",
    "                depth = np.array(depth_img).astype(np.float32)\n",
    "                \n",
    "                # Handle different depth formats (if depth is RGB, take the first channel)\n",
    "                if depth.ndim == 3:\n",
    "                    depth = depth[:, :, 0]  # Take first channel if RGB\n",
    "                \n",
    "                # HyperSim depth_meters.png is usually in meters\n",
    "                depth = np.nan_to_num(depth, nan=0.0, posinf=50.0, neginf=0.0)\n",
    "                \n",
    "                # Some HyperSim depth files are scaled differently (in mm)\n",
    "                if depth.max() > 1000:\n",
    "                    depth = depth / 1000.0  # Convert mm to meters\n",
    "                \n",
    "                # Clip to reasonable range (0.1m to 50m)\n",
    "                depth = np.clip(depth, 0.1, 50.0)\n",
    "                \n",
    "                # Save processed files with set directory name and frame number\n",
    "                # Use set directory name and frame number to avoid conflicts\n",
    "                output_name = f\"{set_dir.name}_frame_{frame_num}.png\"\n",
    "                \n",
    "                # Save RGB image\n",
    "                rgb.save(output_dir / 'rgb' / output_name)\n",
    "                \n",
    "                # Save depth image as 16-bit PNG (preserving precision)\n",
    "                depth_normalized = ((depth - 0.1) / (50.0 - 0.1) * 65535).astype(np.uint16)\n",
    "                Image.fromarray(depth_normalized).save(output_dir / 'depth' / output_name)\n",
    "                \n",
    "                processed_samples.append(output_name)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {depth_file.name}: {e}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\n✓ Processing complete!\")\n",
    "    print(f\"  Successfully processed: {len(processed_samples)} image pairs\")\n",
    "    print(f\"  Skipped: {skipped} frames\")\n",
    "    \n",
    "    if len(processed_samples) == 0:\n",
    "        print(\"\\n ERROR: No samples were processed successfully!\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"  1. Depth files exist: frame.XXXX.depth_meters.png\")\n",
    "        print(\"  2. RGB files exist: frame.XXXX.color.jpg\")\n",
    "        print(\"  3. Frame numbers match between RGB and depth\")\n",
    "    \n",
    "    return processed_samples\n",
    "\n",
    "\n",
    "# Process the dataset\n",
    "dataset_path = Path(r\"dataset\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET PROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if dataset exists\n",
    "if not dataset_path.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at: {dataset_path.absolute()}\")\n",
    "\n",
    "print(f\"✓ Dataset directory found: {dataset_path.absolute()}\")\n",
    "\n",
    "# List some files to verify structure\n",
    "print(\"\\nChecking dataset structure...\")\n",
    "sample_files = list(dataset_path.glob(\"*.png\"))[:5]\n",
    "print(f\"Sample files found:\")\n",
    "for f in sample_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# Process and extract RGB and depth images\n",
    "samples = process_hypersim_scene(dataset_path, output_dir='processed_data')\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise ValueError(\"No samples were processed! Check the error messages above.\")\n",
    "\n",
    "# Create train/val split (80/20)\n",
    "if len(samples) < 2:\n",
    "    print(f\"\\n  Warning: Only {len(samples)} sample(s) found. Using all for training.\")\n",
    "    train_samples = samples\n",
    "    val_samples = samples[:max(1, len(samples)//5)]  # At least 1 sample for validation\n",
    "else:\n",
    "    train_samples, val_samples = train_test_split(samples, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET SPLIT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total samples: {len(samples)}\")\n",
    "print(f\"Training samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")\n",
    "\n",
    "# Save split info\n",
    "with open('train_split.txt', 'w') as f:\n",
    "    f.write('\\n'.join(train_samples))\n",
    "\n",
    "with open('val_split.txt', 'w') as f:\n",
    "    f.write('\\n'.join(val_samples))\n",
    "\n",
    "print(\"\\n✓ Split files saved:\")\n",
    "print(\"  - train_split.txt\")\n",
    "print(\"  - val_split.txt\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161856f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeAttentionModule(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.edge_conv = nn.Sequential(\n",
    "            # Layer 1: Detect basic edges\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Layer 2: Refine edges (NEW)\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Layer 3: Output attention\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Keep Sobel initialization\n",
    "        self._init_as_edge_detector()\n",
    "    \n",
    "    def _init_as_edge_detector(self):\n",
    "        with torch.no_grad():\n",
    "            # Get first conv layer\n",
    "            first_conv = self.edge_conv[0]\n",
    "            \n",
    "            # For each output channel, initialize some filters as Sobel-like\n",
    "            for i in range(min(8, first_conv.out_channels)):\n",
    "                # Horizontal edge detector (Sobel-X inspired)\n",
    "                if i % 2 == 0:\n",
    "                    sobel_x = torch.tensor([\n",
    "                        [-1., 0., 1.],\n",
    "                        [-2., 0., 2.],\n",
    "                        [-1., 0., 1.]\n",
    "                    ])\n",
    "                    first_conv.weight[i, 0, :, :] = sobel_x\n",
    "                # Vertical edge detector (Sobel-Y inspired)\n",
    "                else:\n",
    "                    sobel_y = torch.tensor([\n",
    "                        [-1., -2., -1.],\n",
    "                        [0., 0., 0.],\n",
    "                        [1., 2., 1.]\n",
    "                    ])\n",
    "                    first_conv.weight[i, 0, :, :] = sobel_y\n",
    "    \n",
    "    def forward(self, x):\n",
    "        edge_weights = self.edge_conv(x)\n",
    "        return x * edge_weights\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Standard residual block for stable training\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return F.relu(out + residual)\n",
    "\n",
    "\n",
    "print(\"✓ Edge Attention Module defined!\")\n",
    "print(\"\\nHow it works:\")\n",
    "print(\"  1. Two Conv2d(3x3) layers learn edge patterns\")\n",
    "print(\"  2. Initialized similar to Sobel (but trainable)\")\n",
    "print(\"  3. Outputs attention weights (0=flat, 1=edge)\")\n",
    "print(\"  4. Multiplies features: amplifies edges, suppresses flat regions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85caf36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthNormalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Channel Sequence: 32, 64, 128, 256, 512 (Bottleneck)\n",
    "        \n",
    "        # ===== ENCODER (Downsampling) =====\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 7, padding=3),  # 3 -> 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(32)\n",
    "        )\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 32 -> 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(64),\n",
    "            EdgeAttentionModule(64)\n",
    "        )\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # 64 -> 128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(128),\n",
    "            EdgeAttentionModule(128)\n",
    "        )\n",
    "        \n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),  # 128 -> 256\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(256)\n",
    "        )\n",
    "        \n",
    "        # ===== BOTTLENECK =====\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1),  # 256 -> 512\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(512)\n",
    "        )\n",
    "        \n",
    "        # ===== DECODER (Upsampling) =====\n",
    "        \n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1), # 512 -> 256\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Skip connection size: 256 (dec4 out) + 256 (enc4 skip) = 512 (dec3 in)\n",
    "        \n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, 3, padding=1),  # 512 -> 128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1), # 128 -> 128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Skip connection size: 128 (dec3 out) + 128 (enc3 skip) = 256 (dec2 in)\n",
    "        \n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 64, 3, padding=1),  # 256 -> 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1), # 64 -> 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Skip connection size: 64 (dec2 out) + 64 (enc2 skip) = 128 (dec1 in)\n",
    "        \n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 32, 3, padding=1),  # 128 -> 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 32, 4, stride=2, padding=1), # 32 -> 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Skip connection size: 32 (dec1 out) + 32 (enc1 skip) = 64 (head in)\n",
    "        \n",
    "        # ===== PREDICTION HEADS =====\n",
    "        self.depth_head = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, padding=1),  # 64 -> 32\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, 1), # 32 -> 1\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "        \n",
    "        self.normal_head = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, padding=1), # 64 -> 32\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 3, 1), # 32 -> 3\n",
    "            nn.Tanh()  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder with skip connections\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e4)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d4 = self.dec4(b)\n",
    "        d4 = torch.cat([d4, e4], dim=1) \n",
    "        \n",
    "        d3 = self.dec3(d4)\n",
    "        d3 = torch.cat([d3, e3], dim=1) \n",
    "        \n",
    "        d2 = self.dec2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1) \n",
    "        \n",
    "        d1 = self.dec1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1) \n",
    "        \n",
    "        # Predictions\n",
    "        depth = self.depth_head(d1)\n",
    "        normals = self.normal_head(d1)\n",
    "        \n",
    "        normals = F.normalize(normals, dim=1)\n",
    "        \n",
    "        return depth, normals\n",
    "\n",
    "\n",
    "# Create model and show stats\n",
    "model = DepthNormalNet()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"✓ Model architecture defined!\")\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7270e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeAwareLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss function with edge emphasis\n",
    "    \n",
    "    Components:\n",
    "    1. Depth L1: Basic pixel-wise accuracy\n",
    "    2. Edge Loss: Emphasizes boundaries (2x weight)\n",
    "    3. Normal Loss: Surface orientation accuracy\n",
    "    4. Smoothness: Prevents noise\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_weight=2.0, smooth_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.edge_weight = edge_weight\n",
    "        self.smooth_weight = smooth_weight\n",
    "        \n",
    "        # Fixed Sobel filters for comparison/loss computation\n",
    "        self.register_buffer('sobel_x', torch.tensor([[[\n",
    "            [-1., 0., 1.],\n",
    "            [-2., 0., 2.],\n",
    "            [-1., 0., 1.]\n",
    "        ]]]).float())\n",
    "        \n",
    "        self.register_buffer('sobel_y', torch.tensor([[[\n",
    "            [-1., -2., -1.],\n",
    "            [0., 0., 0.],\n",
    "            [1., 2., 1.]\n",
    "        ]]]).float())\n",
    "    \n",
    "    def compute_gradients(self, img):\n",
    "        \"\"\"Compute image gradients using Sobel filters\"\"\"\n",
    "        grad_x = F.conv2d(img, self.sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(img, self.sobel_y, padding=1)\n",
    "        return torch.sqrt(grad_x**2 + grad_y**2 + 1e-8)\n",
    "    \n",
    "    def forward(self, pred_depth, pred_normal, gt_depth, gt_normal):\n",
    "        # 1. Depth L1 Loss (overall accuracy)\n",
    "        depth_loss = F.l1_loss(pred_depth, gt_depth)\n",
    "        \n",
    "        # 2. Edge Loss (boundary sharpness)\n",
    "        pred_edges = self.compute_gradients(pred_depth)\n",
    "        gt_edges = self.compute_gradients(gt_depth)\n",
    "        edge_loss = F.l1_loss(pred_edges, gt_edges)\n",
    "        \n",
    "        # 3. Normal Loss (surface orientation)\n",
    "        cos_sim = F.cosine_similarity(pred_normal, gt_normal, dim=1)\n",
    "        normal_loss = (1 - cos_sim).mean()\n",
    "        \n",
    "        # 4. Smoothness Loss (noise reduction)\n",
    "        smooth_x = torch.abs(pred_depth[:, :, :, 1:] - pred_depth[:, :, :, :-1])\n",
    "        smooth_y = torch.abs(pred_depth[:, :, 1:, :] - pred_depth[:, :, :-1, :])\n",
    "        smoothness = (smooth_x.mean() + smooth_y.mean()) / 2\n",
    "        \n",
    "        # Combined weighted loss\n",
    "        total_loss = (\n",
    "            depth_loss + \n",
    "            self.edge_weight * edge_loss + \n",
    "            normal_loss + \n",
    "            self.smooth_weight * smoothness\n",
    "        )\n",
    "        \n",
    "        return total_loss, {\n",
    "            'total': total_loss.item(),\n",
    "            'depth': depth_loss.item(),\n",
    "            'edge': edge_loss.item(),\n",
    "            'normal': normal_loss.item(),\n",
    "            'smooth': smoothness.item()\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✓ Loss functions defined!\")\n",
    "print(\"\\nLoss Components:\")\n",
    "print(\"  1. Depth L1 (weight=1.0): Overall accuracy\")\n",
    "print(\"  2. Edge Loss (weight=2.0): Boundary sharpness ← KEY\")\n",
    "print(\"  3. Normal Loss (weight=1.0): Surface orientation\")\n",
    "print(\"  4. Smoothness (weight=0.1): Noise reduction\")\n",
    "print(\"\\nTotal = 1.0×depth + 2.0×edge + 1.0×normal + 0.1×smooth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0de0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperSimDataset(Dataset):\n",
    "    \"\"\"Dataset loader for processed HyperSim data\"\"\"\n",
    "    def __init__(self, data_dir, split_file, img_size=256, augment=False):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Load split\n",
    "        with open(split_file, 'r') as f:\n",
    "            self.samples = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        print(f\"✓ Loaded {len(self.samples)} samples from {split_file}\")\n",
    "    \n",
    "    def depth_to_normals(self, depth):\n",
    "        \"\"\"Compute surface normals from depth map\"\"\"\n",
    "        depth = depth.astype(np.float32)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_x = cv2.Sobel(depth, cv2.CV_32F, 1, 0, ksize=3)\n",
    "        grad_y = cv2.Sobel(depth, cv2.CV_32F, 0, 1, ksize=3)\n",
    "        \n",
    "        # Normal vector: (-dx, -dy, 1)\n",
    "        normal_x = -grad_x\n",
    "        normal_y = -grad_y\n",
    "        normal_z = np.ones_like(depth)\n",
    "        \n",
    "        # Stack and normalize\n",
    "        normals = np.stack([normal_x, normal_y, normal_z], axis=-1)\n",
    "        norm = np.linalg.norm(normals, axis=-1, keepdims=True)\n",
    "        normals = normals / (norm + 1e-8)\n",
    "        \n",
    "        return normals.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_name = self.samples[idx]\n",
    "        \n",
    "        # Load RGB\n",
    "        rgb_path = self.data_dir / 'rgb' / sample_name\n",
    "        rgb = Image.open(rgb_path).convert('RGB')\n",
    "        rgb = rgb.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
    "        rgb = np.array(rgb).astype(np.float32) / 255.0\n",
    "        \n",
    "        # Load depth (16-bit PNG)\n",
    "        depth_path = self.data_dir / 'depth' / sample_name\n",
    "        depth = Image.open(depth_path)\n",
    "        depth = depth.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
    "        depth = np.array(depth).astype(np.float32) / 65535.0  # Back to [0, 1]\n",
    "        \n",
    "        # Compute normals\n",
    "        normals = self.depth_to_normals(depth)\n",
    "        \n",
    "        # Simple augmentation\n",
    "        if self.augment and np.random.rand() > 0.5:\n",
    "            # Horizontal flip\n",
    "            rgb = np.fliplr(rgb).copy()\n",
    "            depth = np.fliplr(depth).copy()\n",
    "            normals = np.fliplr(normals).copy()\n",
    "            normals[:, :, 0] *= -1  # Flip normal X component\n",
    "        \n",
    "        # Convert to tensors\n",
    "        rgb = torch.from_numpy(rgb).permute(2, 0, 1).float()\n",
    "        depth = torch.from_numpy(depth).unsqueeze(0).float()\n",
    "        normals = torch.from_numpy(normals).permute(2, 0, 1).float()\n",
    "        \n",
    "        # Normalize RGB to [-1, 1]\n",
    "        rgb = rgb * 2 - 1\n",
    "        \n",
    "        return rgb, depth, normals\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HyperSimDataset('processed_data', 'train_split.txt', \n",
    "                                img_size=512, augment=True)\n",
    "val_dataset = HyperSimDataset('processed_data', 'val_split.txt', \n",
    "                              img_size=512, augment=False)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE =4  # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                         shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n",
    "                       shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"\\n✓ Dataloaders ready!\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    metrics = {'depth': 0, 'edge': 0, 'normal': 0, 'smooth': 0}\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f'Epoch {epoch} - Training')\n",
    "    for batch_idx, (rgb, depth, normals) in enumerate(pbar):\n",
    "        rgb = rgb.to(device)\n",
    "        depth = depth.to(device)\n",
    "        normals = normals.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_depth, pred_normals = model(rgb)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, loss_dict = criterion(pred_depth, pred_normals, depth, normals)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (prevents exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        for k in ['depth', 'edge', 'normal', 'smooth']:\n",
    "            metrics[k] += loss_dict[k]\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'edge': f'{loss_dict[\"edge\"]:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Average metrics\n",
    "    n = len(loader)\n",
    "    return total_loss / n, {k: v / n for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device, epoch):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    metrics = {'depth': 0, 'edge': 0, 'normal': 0, 'smooth': 0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=f'Epoch {epoch} - Validation')\n",
    "        for rgb, depth, normals in pbar:\n",
    "            rgb = rgb.to(device)\n",
    "            depth = depth.to(device)\n",
    "            normals = normals.to(device)\n",
    "            \n",
    "            pred_depth, pred_normals = model(rgb)\n",
    "            loss, loss_dict = criterion(pred_depth, pred_normals, depth, normals)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            for k in ['depth', 'edge', 'normal', 'smooth']:\n",
    "                metrics[k] += loss_dict[k]\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    n = len(loader)\n",
    "    return total_loss / n, {k: v / n for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "print(\"✓ Training functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae86056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Model\n",
    "model = DepthNormalNet().to(device)\n",
    "print(f\"Model loaded to {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\\n\")\n",
    "\n",
    "# Loss and optimizer\n",
    "NUM_EPOCHS = 15\n",
    "criterion = EdgeAwareLoss(edge_weight=2.0, smooth_weight=0.1)\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=3e-4,  # Peak learning rate\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3,  # Warmup for 30% of training\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "# Training settings\n",
    "best_val_loss = float('inf')\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_edge': [], 'val_edge': [],\n",
    "    'train_depth': [], 'val_depth': [],\n",
    "    'train_normal': [], 'val_normal': []\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: 1e-4\")\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"Scheduler: CosineAnnealingLR\")\n",
    "print(f\"Loss weights: depth=1.0, edge=2.0, normal=1.0, smooth=0.1\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EPOCH {epoch}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_metrics = train_epoch(model, train_loader, optimizer, \n",
    "                                           criterion, device, epoch)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics = validate(model, val_loader, criterion, device, epoch)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nEpoch {epoch} Results:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Train Depth: {train_metrics['depth']:.4f} | Val Depth: {val_metrics['depth']:.4f}\")\n",
    "    print(f\"  Train Edge: {train_metrics['edge']:.4f} | Val Edge: {val_metrics['edge']:.4f} ← KEY METRIC\")\n",
    "    print(f\"  Train Normal: {train_metrics['normal']:.4f} | Val Normal: {val_metrics['normal']:.4f}\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Track history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_edge'].append(train_metrics['edge'])\n",
    "    history['val_edge'].append(val_metrics['edge'])\n",
    "    history['train_depth'].append(train_metrics['depth'])\n",
    "    history['val_depth'].append(val_metrics['depth'])\n",
    "    history['train_normal'].append(train_metrics['normal'])\n",
    "    history['val_normal'].append(val_metrics['normal'])\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss,\n",
    "            'history': history\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"  ✓ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, f'checkpoint_epoch_{epoch}.pth')\n",
    "        print(f\"  ✓ Saved checkpoint\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e912b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Total loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Total Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training Progress - Total Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history['train_edge'], label='Train Edge Loss', marker='o', \n",
    "                linewidth=2, color='red')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Edge Loss', fontsize=12)\n",
    "axes[0, 1].set_title('Edge Loss Over Time (KEY METRIC)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Depth loss\n",
    "axes[1, 0].plot(history['train_depth'], label='Train Depth Loss', marker='o', \n",
    "                linewidth=2, color='blue')\n",
    "axes[1, 0].plot(history['val_depth'], label='Val Depth Loss', marker='s', \n",
    "                linewidth=2, color='darkblue')\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Depth Loss', fontsize=12)\n",
    "axes[1, 0].set_title('Depth Loss Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training curves saved as 'training_curves.png'\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(f\"  Initial edge loss: {history['train_edge'][0]:.4f}\")\n",
    "print(f\"  Final edge loss: {history['train_edge'][-1]:.4f}\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d753e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output_rgba(pred_depth, pred_normals, output_path='output.png'):\n",
    "    \"\"\"\n",
    "    Save RGBA PNG: RGB channels = Normal map, Alpha channel = Depth map\n",
    "    This is the production format for 3D artists\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    normals = pred_normals.squeeze(0).cpu().numpy()  # [3, H, W]\n",
    "    depth = pred_depth.squeeze().cpu().numpy()  # [H, W]\n",
    "    \n",
    "    # Encode normals: [-1, 1] → [0, 255]\n",
    "    normals = ((normals + 1) * 0.5 * 255).astype(np.uint8)\n",
    "    normals = np.transpose(normals, (1, 2, 0))  # [H, W, 3]\n",
    "    normals = np.clip(normals, 0, 255)\n",
    "    \n",
    "    # Encode depth: [0, 1] → [0, 255]\n",
    "    depth = (depth * 255).astype(np.uint8)\n",
    "    depth = np.expand_dims(depth, -1)  # [H, W, 1]\n",
    "    \n",
    "    # Combine RGBA\n",
    "    rgba = np.concatenate([normals, depth], axis=-1)  # [H, W, 4]\n",
    "    \n",
    "    # Save as PNG\n",
    "    img = Image.fromarray(rgba, mode='RGBA')\n",
    "    img.save(output_path, compress_level=6)\n",
    "    \n",
    "    file_size = os.path.getsize(output_path) / 1024\n",
    "    print(f\"✓ Saved RGBA output: {output_path}\")\n",
    "    print(f\"  Resolution: {rgba.shape[1]}×{rgba.shape[0]}\")\n",
    "    print(f\"  File size: {file_size:.1f} KB\")\n",
    "    print(f\"  Format: RGBA PNG (lossless)\")\n",
    "    print(f\"  RGB channels: Normal map (tangent-space)\")\n",
    "    print(f\"  Alpha channel: Depth map (normalized)\")\n",
    "    \n",
    "    return rgba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Training Results:\")\n",
    "print(f\"  Total epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Final training loss: {history['train_loss'][-1]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_custom_image(image_path, model, device, output_path='custom_output.png'):\n",
    "    \"\"\"\n",
    "    Test the trained model on a custom image\n",
    "    \n",
    "    Usage:\n",
    "    test_custom_image('path/to/your/image.jpg', model, device)\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing custom image: {image_path}\")\n",
    "    \n",
    "    # Load and preprocess\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    original_size = img.size\n",
    "    img_resized = img.resize((512, 512), Image.BILINEAR)\n",
    "    \n",
    "    img_array = np.array(img_resized).astype(np.float32) / 255.0\n",
    "    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float()\n",
    "    img_tensor = img_tensor * 2 - 1  # Normalize to [-1, 1]\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_depth, pred_normals = model(img_tensor)\n",
    "    \n",
    "    # Save RGBA\n",
    "    rgba = save_output_rgba(pred_depth, pred_normals, output_path)\n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    axes[0].imshow(img_resized)\n",
    "    axes[0].set_title('Input Image', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(pred_depth.squeeze().cpu().numpy(), cmap='plasma')\n",
    "    axes[1].set_title('Predicted Depth', fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    pred_normals_display = (pred_normals.squeeze().cpu().permute(1, 2, 0).numpy() + 1) / 2\n",
    "    axes[2].imshow(pred_normals_display)\n",
    "    axes[2].set_title('Predicted Normals', fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    axes[3].imshow(rgba)\n",
    "    axes[3].set_title('RGBA Output', fontsize=14, fontweight='bold')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('custom_result.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Custom image processed!\")\n",
    "    print(f\"  Input size: {original_size}\")\n",
    "    print(f\"  Processed at: 512×512\")\n",
    "    print(f\"  Output saved: {output_path}\")\n",
    "    print(f\"  Visualization saved: custom_result.png\")\n",
    "\n",
    "test_custom_image(\"brick.jpg\", model, device)\n",
    "test_custom_image(\"brick2.jpg\", model, device)\n",
    "test_custom_image(\"brick4.jpg\", model, device)\n",
    "test_custom_image(\"brick5.jpg\", model, device)\n",
    "test_custom_image(\"check.png\", model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
